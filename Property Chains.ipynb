{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdflib:RDFLib Version: 4.2.1\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from rdflib import URIRef\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "import gzip\n",
    "import scipy\n",
    "import random\n",
    "import json\n",
    "import scipy.sparse\n",
    "import csv\n",
    "import rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self):\n",
    "        self.data2num = {}\n",
    "        self.num2data = []\n",
    "    \n",
    "    def add(self, value):\n",
    "        if value not in self.data2num:\n",
    "            v = len(self.num2data)\n",
    "            self.data2num[value] = v\n",
    "            self.num2data.append(value)\n",
    "            return v\n",
    "        else:\n",
    "            return self(value)\n",
    "    \n",
    "    def __call__(self, value):\n",
    "        return self.data2num[value]\n",
    "    \n",
    "    def __getitem__(self, num):\n",
    "        return self.num2data[num]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.num2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 500000\n",
      "Line 1000000\n",
      "Line 1500000\n",
      "Line 2000000\n",
      "Line 2500000\n",
      "Line 3000000\n",
      "Line 3500000\n",
      "Line 4000000\n",
      "Line 4500000\n",
      "Line 5000000\n",
      "Line 5500000\n",
      "Line 6000000\n",
      "Line 6500000\n",
      "Line 7000000\n",
      "Line 7500000\n",
      "Line 8000000\n",
      "Line 8500000\n",
      "Line 9000000\n",
      "Line 9500000\n",
      "Line 10000000\n",
      "Line 10500000\n",
      "Line 11000000\n",
      "Line 11500000\n",
      "Line 12000000\n",
      "Line 12500000\n",
      "Line 13000000\n",
      "Line 13500000\n",
      "Line 14000000\n",
      "Line 14500000\n",
      "Line 15000000\n",
      "Line 15500000\n",
      "Line 16000000\n",
      "Line 16500000\n",
      "Line 17000000\n",
      "Line 17500000\n",
      "Line 18000000\n",
      "Line 18500000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18111905"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = []\n",
    "\n",
    "soenc = Encoder()\n",
    "penc = Encoder()\n",
    "\n",
    "with gzip.open('DBpedia201610/mappingbased_objects_en.ttl.gz', 'rt') as f:\n",
    "    for n,line in enumerate(f):\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "#         if ignored_triples == 100:\n",
    "#             break\n",
    "        if (n+1)%500000 == 0:\n",
    "            print(\"Line\", n+1)\n",
    "        line = line.strip()\n",
    "        line = line[:-1]\n",
    "        line = line.split()\n",
    "        assert len(line) == 3\n",
    "        if line[1].startswith(\"<http://dbpedia.org/ontology/\"):            \n",
    "            s = soenc.add(line[0])\n",
    "            p = penc.add(line[1])\n",
    "            o = soenc.add(line[2])\n",
    "            graph.append((s, p, o))            \n",
    "            \n",
    "len(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spo = defaultdict(lambda:defaultdict(lambda:set()))\n",
    "sop = defaultdict(lambda:defaultdict(lambda:set()))\n",
    "ops = defaultdict(lambda:defaultdict(lambda:set()))\n",
    "for s, p, o in graph:\n",
    "    spo[s][p].add(o)\n",
    "    sop[s][o].add(p)\n",
    "    ops[o][p].add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Inverse = namedtuple('Inverse', ['base'])\n",
    "\n",
    "class SparseIdx:\n",
    "    def __init__(self, graph, n_so, n_p):\n",
    "        tmp = [scipy.sparse.dok_matrix((n_so, n_so)) for _ in range(n_p)]\n",
    "        for s, p, o in graph:\n",
    "            tmp[p][s, o] = 1\n",
    "        self.pso_bin = [m.tocsr() for m in tmp]\n",
    "        self.pos_bin = [m.transpose().tocsr() for m in tmp]\n",
    "        \n",
    "    def _query(self, *paths):\n",
    "        def helper(p):\n",
    "            if isinstance(p, Inverse):\n",
    "                if isinstance(p.base, Inverse):\n",
    "                    return helper(p.base.base)\n",
    "                else:\n",
    "                    return self.pos_bin[p.base]\n",
    "            else:\n",
    "                return self.pso_bin[p]\n",
    "        result = None\n",
    "        for path in paths:\n",
    "            partial = helper(path[0])\n",
    "            for p in path[1:]:\n",
    "                partial @= helper(p)\n",
    "            if result is None:\n",
    "                result = partial\n",
    "            else:\n",
    "                result = result.multiply(partial)\n",
    "        return result\n",
    "    \n",
    "    def count(self, *paths):\n",
    "        return self._query(*paths).getnnz()\n",
    "    \n",
    "    def query(self, *paths):\n",
    "        tmp = self._query(*paths)\n",
    "        return [(s, o) for s, o in zip(*tmp.nonzero())]\n",
    "    \n",
    "    def pos_neg_query(self, pos_paths, neg_paths):    \n",
    "        pos = self._query(*pos_paths)\n",
    "        neg = self._query(*neg_paths)\n",
    "        tmp = pos - neg.multiply(pos)\n",
    "        return [(s, o) for s, o in zip(*tmp.nonzero())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pidx = SparseIdx(graph, len(soenc), len(penc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triples_on_path(s, path, o):\n",
    "    current = s\n",
    "    p = path[0]\n",
    "    if len(path) == 1:\n",
    "        if o in spo[s][p]:\n",
    "            return [[(s, p, o)]]\n",
    "    else:\n",
    "        result = []\n",
    "        for t in spo[s][p]:\n",
    "            result += [[(s, p, t)] + triples for triples in triples_on_path(t, path[1:], o) if len(triples) > 0]            \n",
    "        return result\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 500000\n",
      "Line 1000000\n",
      "Line 1500000\n",
      "Line 2000000\n",
      "Line 2500000\n",
      "Line 3000000\n",
      "Line 3500000\n",
      "Line 4000000\n",
      "Line 4500000\n",
      "Line 5000000\n",
      "Line 5500000\n",
      "Line 6000000\n",
      "Line 6500000\n",
      "Line 7000000\n",
      "Line 7500000\n",
      "Line 8000000\n",
      "Line 8500000\n",
      "Line 9000000\n",
      "Line 9500000\n",
      "Line 10000000\n",
      "Line 10500000\n",
      "Line 11000000\n",
      "Line 11500000\n",
      "Line 12000000\n",
      "Line 12500000\n"
     ]
    }
   ],
   "source": [
    "so_labels = [None]*len(soenc)\n",
    "p_labels = [None]*len(penc)\n",
    "\n",
    "with gzip.open('DBpedia201610/labels_en.ttl.gz', 'rt') as f:\n",
    "    for n, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        if line[0] == '#' or len(line) == 0:\n",
    "            continue\n",
    "        if (n+1)%500000 == 0:\n",
    "            print(\"Line\", n+1)\n",
    "        line = line.strip()\n",
    "        line = line[:-1]\n",
    "        line = line.split(maxsplit=2)        \n",
    "        assert len(line) == 3\n",
    "        assert line[1] == '<http://www.w3.org/2000/01/rdf-schema#label>'\n",
    "        try:\n",
    "            s = soenc(line[0])        \n",
    "        except KeyError:\n",
    "            continue        \n",
    "        assert line[2][0] == '\"'\n",
    "        e = line[2].rindex('\"')\n",
    "        l = line[2][1:e]        \n",
    "        assert so_labels[s] is None or so_labels[s] == l, (s, so_labels[s], line)\n",
    "        so_labels[s] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "ontology = rdflib.Graph()\n",
    "ontology.parse('DBpedia201610/dbpedia_2016-10.owl', format='xml')\n",
    "\n",
    "for p in range(len(penc)):\n",
    "    uri = URIRef(penc[p][1:-1])\n",
    "    labels = ontology.preferredLabel(uri, lang='en')\n",
    "    assert len(labels) >= 1\n",
    "    label = str(labels[0][1])\n",
    "    p_labels[p] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shorten(struct):\n",
    "    if isinstance(struct, str):\n",
    "        if struct[0] == '<' and struct[-1] == '>':\n",
    "            prefixes = [('dbo:', 'http://dbpedia.org/ontology/'), ('dbr:', 'http://dbpedia.org/resource/')]\n",
    "            struct = struct[1:-1]\n",
    "            for p, ns in prefixes:\n",
    "                if struct.startswith(ns):\n",
    "                    return p + struct[len(ns):]\n",
    "        return struct\n",
    "    else:\n",
    "        return [shorten(item) for item in struct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-----------------\n",
    "Idea: wyszukujemy częste wzorce przechodząc od grupy obiektów do grupy obiektów, w sumie podobnie jak SLDM.\n",
    "Obiekty muszą być ważone, tak jak to było w SLDM, żeby poradzić sobie z \"zapadnieciem sie sciezki\", np. (si, o) i=1...100\n",
    "\n",
    "Próby jednoczesnego liczenia odwrotności są zbyt kosztowne, *ale*: odwrotność properties to transpozycja macierzy, a rozpoznawanie sprowadzi się wtedy do zliczania NNZ w point-wise iloczynie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_frequent_patterns(S, O, min_sup, max_len, accept, reject, current_path = []):        \n",
    "    if accept(current_path):\n",
    "        args = [S[s] for s in S.keys() & O]\n",
    "        if len(args) >= 1:\n",
    "            sup_set = set.union(*args)\n",
    "            sup = len(sup_set)\n",
    "            if sup >= min_sup:\n",
    "                return [(current_path, sup_set)]\n",
    "    if reject(current_path):\n",
    "        return []\n",
    "    if max_len == 0:\n",
    "        return []\n",
    "    ctr = defaultdict(lambda:defaultdict(lambda: set()))\n",
    "    for s, sup_set in S.items():\n",
    "        for p, objs in spo[s].items():\n",
    "            for o in objs:\n",
    "                ctr[p][o] |= sup_set        \n",
    "#         for p, subjs in ops[s].items():\n",
    "#             for o in subjs:\n",
    "#                 ctr[Inverse(p)][o] |= sup_set\n",
    "    result = []\n",
    "    for p, objs in ctr.items():\n",
    "        sup = len(set.union(*objs.values()))    \n",
    "        if sup >= min_sup:\n",
    "            result += find_frequent_patterns(objs, O, min_sup, max_len-1, accept, reject, current_path+[p])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_unwanted(p):\n",
    "    l = p_labels[p]\n",
    "    keywords = ['spouse', 'successor', 'predecessor', 'previous', 'following', 'subsequent']\n",
    "    return any([k in l for k in keywords])\n",
    "\n",
    "unwanted = {p for p in range(len(penc)) if is_unwanted(p)}\n",
    "\n",
    "def reject(path):\n",
    "    return len(path) > 0 and path[-1] in unwanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(r, cfg):\n",
    "    max_len = cfg['max_len']\n",
    "    min_sup = cfg['min_sup']\n",
    "    min_conf = cfg['min_conf']\n",
    "    r_path = [r]\n",
    "    \n",
    "    S = {}\n",
    "    O = set()\n",
    "    for s, o in pidx.query(r_path):\n",
    "        S[s] = {s}\n",
    "        O.add(o)\n",
    "        \n",
    "    accept = lambda path: len(path) >= 1 and (path[0] != r or len(path) >= 2)    \n",
    "    FP = find_frequent_patterns(S, O, min_sup, max_len, accept, reject)\n",
    "    \n",
    "    result = []    \n",
    "    for path, sup_set in FP:        \n",
    "        if len(path) > 2 and path[0] == path[-1]:\n",
    "            continue    \n",
    "        path_sup = pidx.count(path)\n",
    "        sup = pidx.count(path, [r])\n",
    "        counterevidence = pidx.pos_neg_query([path], [r_path])\n",
    "        conf = sup / path_sup\n",
    "        #print([shorten(penc[p]) for p in path], sup, conf)\n",
    "        if conf >= min_conf:\n",
    "            desc = {          \n",
    "                'support': sup,\n",
    "                'confidence': conf,\n",
    "                'counterevidence': counterevidence\n",
    "            }\n",
    "            result.append((path, desc))\n",
    "    return result\n",
    "\n",
    "#process(penc('<http://dbpedia.org/ontology/isPartOf>'), {'max_len': 2, 'min_sup': 10000, 'min_conf': .95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with concurrent.futures.ProcessPoolExecutor(4) as executor:           \n",
    "    cfg = {'max_len': 2, 'min_sup': 100, 'min_conf': .95}\n",
    "    properties = list(range(len(penc)))        \n",
    "    results = executor.map(process, properties, [cfg]*len(penc))\n",
    "    all_paths = dict(zip(properties, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_paths = {r: {tuple(path): info for path, info in paths} for r, paths in all_paths.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decoded = {}\n",
    "\n",
    "for r, paths in all_paths.items():\n",
    "    if len(paths) == 0:\n",
    "        continue\n",
    "    decoded_paths = []    \n",
    "    for path, info in paths.items():\n",
    "        key = tuple((penc[p] for p in path))\n",
    "        ce = list([(soenc[s], soenc[o]) for s, o in info['counterevidence']])\n",
    "        decoded_paths.append((key, {'support': info['support'],\n",
    "                                                  'confidence': info['confidence'],\n",
    "                                                  'counterevidence': ce\n",
    "                                                 }))\n",
    "    decoded[penc[r]] = decoded_paths        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = 'decoded-{}.json.gz'.format(datetime.now())\n",
    "\n",
    "with gzip.open(fn, 'wt') as f:\n",
    "    json.dump({'cfg': cfg, 'decoded': decoded}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
